{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOPkZ7pAH1hD"
   },
   "source": [
    "# Objective: Forest vs Building Classification Using ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wrqhxNtH1hM"
   },
   "source": [
    "Steps:\n",
    "1. Importing (or installing) Tenosrflow, Keras and other packages on your system\n",
    "2. Loading your data from disk\n",
    "3. Creating your training and testing splits\n",
    "4. Data Preprocessing \n",
    "5. Defining your tensorflow ANN model architecture\n",
    "6. Compiling your tensorflow ANN model\n",
    "7. Training your model on your training data\n",
    "8. Evaluating your model on your test data\n",
    "9. Generate Plots for accuracy and validation loss\n",
    "10. Saving The train model\n",
    "11. Making predictions using your trained tensorflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dd-17cKpH1hO"
   },
   "source": [
    "### Step 1: Importing all the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Wc07XIN8H1hP"
   },
   "outputs": [],
   "source": [
    "# Import libraries and packages\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix , accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time   # time1 = time.time(); print('Time taken: {:.1f} seconds'.format(time.time() - time1))\n",
    "import warnings\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import itertools\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42   # set random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4ykOP9PH1hR"
   },
   "source": [
    "### Step 2: Loading your data from disk for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mount google drive to collab notebook\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # change working DIR \n",
    "# import os \n",
    "# os.chdir(\"/content/drive/MyDrive/Imarticus_Deep_learning_B2/DAY_1_ANN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dataset.rar file\n",
    "# !pip install patool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import patoolib\n",
    "# patoolib.extract_archive(\"dataset.rar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patoolib.extract_archive(\"test_examples.rar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "edc6e57fb04c4d8b8a479d6cbc6a475a"
     ]
    },
    "id": "C6fWvIhpH1hS",
    "outputId": "fd5905d7-bdb7-4567-b98b-a18599db0685"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efc3d7201bf400185212b329994030f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1.3 seconds\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# initialize the data and labels\n",
    "print(\"[INFO] loading images...\")\n",
    "time1 = time.time()   # to measure time taken\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "classes = [\"Forest\", \"Buildings\"]\n",
    "\n",
    "# grab the image paths and randomly shuffle them\n",
    "imagePaths = sorted(list(paths.list_images('dataset')))   # data folder with 2 categorical folders\n",
    "random.seed(SEED)\n",
    "random.shuffle(imagePaths)\n",
    "\n",
    "# progress bar \n",
    "with tqdm(total=len(imagePaths)) as pbar:\n",
    "    \n",
    "    # loop over the input images\n",
    "    for imagePath in imagePaths:\n",
    "        # load the image, resize the image to be 32x32 pixels (ignoring aspect ratio), \n",
    "        # flatten the 32x32x3=3072 pixel image into a list, and store the image in the data list\n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.resize(image, (32, 32)).flatten()\n",
    "        data.append(image)\n",
    "\n",
    "        # extract the class label from the image path and update the labels list\n",
    "        label = imagePath.split(os.path.sep)[-2]\n",
    "        \n",
    "        label = 1 if label == \"Buildings\" else 0\n",
    "        labels.append(label)\n",
    "        \n",
    "        # update the progressbar\n",
    "        pbar.update(1)\n",
    "\n",
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('Time taken: {:.1f} seconds'.format(time.time() - time1))   # to measure time taken\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nVPfN9uLH1hV",
    "outputId": "663e606a-4b4e-4199-8e61-738c87c427ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images:  883\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Images: \", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubSNZ3sXH1hW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4t61EG6aH1hW",
    "outputId": "944b00af-3eda-461b-96e5-a1b4053df781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample image: [0.31764706 0.29803922 0.28627451 ... 0.74117647 0.58823529 0.4627451 ]\n",
      "no of features/pixels values: 3072\n",
      "label: Forest\n"
     ]
    }
   ],
   "source": [
    "# sample data for first image\n",
    "print(\"sample image: {}\".format(data[0]))\n",
    "print(\"no of features/pixels values: {}\".format(len(data[0]))) # 32x32x3=3072\n",
    "print(\"label: {}\".format(classes[labels[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1om_oqHH1hX"
   },
   "source": [
    "### Step 3: Creating your training and testing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0JeZsctwH1hY"
   },
   "outputs": [],
   "source": [
    "# partition the data into 80% training and 20% validation\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LS-WxAlMH1hZ",
    "outputId": "908c7c4f-f56c-48db-a5fc-0d86c9575bb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(706, 3072)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_HgWZSTVH1hZ",
    "outputId": "72ef7e3f-6c92-447e-fe58-6a747bb861a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(706,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "383Ph1VjH1ha",
    "outputId": "d0a45738-3ea4-4469-875f-4cadfda1e6b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177, 3072)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1uUniwLqH1ha",
    "outputId": "b4a818f0-d2fb-4d39-ec76-aba41428880e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EZUchGj5H1hb",
    "outputId": "5b66c436-e549-42e1-d898-93ff9aa7eabb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20392157, 0.57254902, 0.34509804, ..., 0.00784314, 0.17254902,\n",
       "        0.07843137],\n",
       "       [0.85490196, 0.87843137, 0.95686275, ..., 0.        , 0.        ,\n",
       "        0.01176471],\n",
       "       [0.76862745, 0.77254902, 0.78039216, ..., 0.44313725, 0.46666667,\n",
       "        0.48627451],\n",
       "       ...,\n",
       "       [0.88235294, 0.87058824, 0.80392157, ..., 0.21960784, 0.19215686,\n",
       "        0.23137255],\n",
       "       [0.84313725, 0.78823529, 0.79215686, ..., 0.3372549 , 0.36862745,\n",
       "        0.62745098],\n",
       "       [0.02352941, 0.35686275, 0.17254902, ..., 0.09411765, 0.28627451,\n",
       "        0.21568627]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XOQ2xVXWH1he",
    "outputId": "6d133ab9-f28c-46e4-ef72-f3cbb52fe75d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bGXcQ96NH1he",
    "outputId": "9cf333bf-711f-444c-dab1-2ced6c2abb4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "S2xgBvbyH1hf",
    "outputId": "2dc3348e-1b7b-4700-b925-f572daf723b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ju5PeM9IH1hf"
   },
   "source": [
    "### Step 4: Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KOVY7aNvH1hg"
   },
   "outputs": [],
   "source": [
    "# convert the labels from integers/categories to vectors \n",
    "\n",
    "trainY = to_categorical(trainY, num_classes=2)   # fit_transform = find all unique class labels + transform into one-hot encoded labels\n",
    "testY = to_categorical(testY, num_classes=2)     # transform = perform the one-hot encoding (unique class labels already found)\n",
    "\n",
    "# [0,1] Buildings\n",
    "# [1,0] Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "uSIXmTiAH1hg"
   },
   "outputs": [],
   "source": [
    "# testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6a8Kt4OAH1hg",
    "outputId": "9b7d64d1-fd93-4236-c9a4-2fa2d9984d6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sqa_ionjH1hh"
   },
   "outputs": [],
   "source": [
    "sample_image = (trainX[5] * 255).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "6CvUQMryH1hh",
    "outputId": "2d3585a7-3e5f-44a7-84d9-70a94c11b06d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28836b95488>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample_image.reshape(32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6M9W37ywH1hh",
    "outputId": "5aa2421a-753e-4a7e-a12d-b34ad91ea9e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[5]   # [0,1] means buildings [1,0] means forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWaKJJQAH1hi"
   },
   "source": [
    "### Step 5:  Define the architecture for ANN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "sn6MdCz1H1hi"
   },
   "outputs": [],
   "source": [
    "# define the 3072-1024-512-1 architecture using Keras\n",
    "\n",
    "model = Sequential()   \n",
    "\n",
    "# input layer 3072 as there are 32x32x3=3072 pixels in a flattened input image\n",
    "# first hidden layer has 1024 nodes\n",
    "model.add(Dense(units= 1024, input_shape=(3072,), kernel_initializer = 'uniform', activation=\"relu\"))  \n",
    "\n",
    "# # dropout for second layer \n",
    "# model.add(Dropout(0.4))\n",
    "\n",
    "# second hidden layer has 512 nodes\n",
    "model.add(Dense(units=512, kernel_initializer='uniform', activation=\"relu\"))                         \n",
    "\n",
    " # output layer with number of possible class labels\n",
    "model.add(Dense(units=2,kernel_initializer='uniform', activation=\"softmax\"))           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4TsihI_H1hi"
   },
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtOX9yVrH1hi"
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH_Z8uwLH1hj"
   },
   "source": [
    "### Step 6:  Compiling your tensorflow ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4RfbU0kQH1hj",
    "outputId": "634fd727-b75f-4466-a688-3a224638672f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling network network...\n"
     ]
    }
   ],
   "source": [
    "# initialize our initial learning rate and # of epochs to train for\n",
    "INIT_LR = 0.01\n",
    "EPOCHS = 50\n",
    " \n",
    "# compile the model using SGD as our optimizer and categorical cross-entropy loss\n",
    "# (you'll want to use binary_crossentropy for 2-class classification)\n",
    "print(\"[INFO] compiling network network...\")\n",
    "opt = SGD(lr=INIT_LR)   # Stochastic Gradient Descent (SGD) optimizer\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "6YUcdkObH1hj",
    "outputId": "dab436fa-23cc-4707-9a6c-e0f4f6bd0164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1024)              3146752   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,672,578\n",
      "Trainable params: 3,672,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2VDhQ7tH1hk"
   },
   "source": [
    "### Step 7: Training your model on your training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLiN6EKYH1hk"
   },
   "source": [
    "#### Fit (ie, Train) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4JM-4rOxH1hk",
    "outputId": "3039f2ec-0276-41e0-d48f-97b8af223b49",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000028837FFC558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000028837FFC558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6511 - accuracy: 0.5567WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000028837F8BAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000028837F8BAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "23/23 [==============================] - 8s 52ms/step - loss: 0.6511 - accuracy: 0.5567 - val_loss: 0.6455 - val_accuracy: 0.5989\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 1s 35ms/step - loss: 0.6226 - accuracy: 0.5722 - val_loss: 0.6131 - val_accuracy: 0.5537\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 0.5993 - accuracy: 0.5793 - val_loss: 0.5965 - val_accuracy: 0.5537\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 0.5789 - accuracy: 0.6331 - val_loss: 0.5643 - val_accuracy: 0.7175\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 0.5565 - accuracy: 0.6983 - val_loss: 0.5413 - val_accuracy: 0.7966\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 0.5332 - accuracy: 0.7535 - val_loss: 0.5437 - val_accuracy: 0.6384\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 1s 38ms/step - loss: 0.5155 - accuracy: 0.7535 - val_loss: 0.5139 - val_accuracy: 0.6949\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.4955 - accuracy: 0.7649 - val_loss: 0.4702 - val_accuracy: 0.8531\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 0.4726 - accuracy: 0.7932 - val_loss: 0.4494 - val_accuracy: 0.8644\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 1s 38ms/step - loss: 0.4525 - accuracy: 0.8343 - val_loss: 0.5159 - val_accuracy: 0.7458\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 1s 39ms/step - loss: 0.4428 - accuracy: 0.8357 - val_loss: 0.4452 - val_accuracy: 0.8588\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.4265 - accuracy: 0.8484 - val_loss: 0.4072 - val_accuracy: 0.8136\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 1s 34ms/step - loss: 0.4087 - accuracy: 0.8258 - val_loss: 0.4065 - val_accuracy: 0.8023\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 1s 27ms/step - loss: 0.3957 - accuracy: 0.8484 - val_loss: 0.3823 - val_accuracy: 0.8757\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 1s 28ms/step - loss: 0.3824 - accuracy: 0.8555 - val_loss: 0.3822 - val_accuracy: 0.8192\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 1s 34ms/step - loss: 0.3733 - accuracy: 0.8527 - val_loss: 0.4087 - val_accuracy: 0.7966\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 1s 38ms/step - loss: 0.3732 - accuracy: 0.8428 - val_loss: 0.3647 - val_accuracy: 0.8418\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.3608 - accuracy: 0.8626 - val_loss: 0.4092 - val_accuracy: 0.7910\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.3519 - accuracy: 0.8626 - val_loss: 0.3487 - val_accuracy: 0.8588\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 1s 37ms/step - loss: 0.3382 - accuracy: 0.8669 - val_loss: 0.4517 - val_accuracy: 0.7627\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 1s 33ms/step - loss: 0.3382 - accuracy: 0.8640 - val_loss: 0.5513 - val_accuracy: 0.7062\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 1s 34ms/step - loss: 0.3417 - accuracy: 0.8584 - val_loss: 0.3296 - val_accuracy: 0.8814\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.3218 - accuracy: 0.8782 - val_loss: 0.3561 - val_accuracy: 0.8305\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 0.3162 - accuracy: 0.8824 - val_loss: 0.3633 - val_accuracy: 0.8192\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 1s 28ms/step - loss: 0.3025 - accuracy: 0.8711 - val_loss: 0.3308 - val_accuracy: 0.8588\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 1s 27ms/step - loss: 0.2932 - accuracy: 0.8782 - val_loss: 0.3161 - val_accuracy: 0.8870\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 0.2904 - accuracy: 0.8881 - val_loss: 0.3411 - val_accuracy: 0.8305\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 0.2797 - accuracy: 0.8966 - val_loss: 0.4657 - val_accuracy: 0.7458\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.2822 - accuracy: 0.8924 - val_loss: 0.3222 - val_accuracy: 0.8927\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 1s 28ms/step - loss: 0.2715 - accuracy: 0.8994 - val_loss: 0.5819 - val_accuracy: 0.7684\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.2855 - accuracy: 0.9023 - val_loss: 0.5296 - val_accuracy: 0.7232\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 1s 32ms/step - loss: 0.2576 - accuracy: 0.9136 - val_loss: 0.3458 - val_accuracy: 0.8531\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.2560 - accuracy: 0.9235 - val_loss: 0.3202 - val_accuracy: 0.8814\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 1s 33ms/step - loss: 0.2565 - accuracy: 0.9164 - val_loss: 0.3624 - val_accuracy: 0.8362\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 0.2533 - accuracy: 0.9150 - val_loss: 1.5415 - val_accuracy: 0.4859\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 0.3319 - accuracy: 0.8796 - val_loss: 0.3407 - val_accuracy: 0.8531\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 0.2266 - accuracy: 0.9306 - val_loss: 0.3081 - val_accuracy: 0.8814\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 1s 28ms/step - loss: 0.2218 - accuracy: 0.9292 - val_loss: 0.3019 - val_accuracy: 0.8757\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 1s 30ms/step - loss: 0.2225 - accuracy: 0.9263 - val_loss: 1.7025 - val_accuracy: 0.4802\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 1s 28ms/step - loss: 0.2788 - accuracy: 0.9150 - val_loss: 0.3075 - val_accuracy: 0.8757\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 0.2046 - accuracy: 0.9462 - val_loss: 0.3069 - val_accuracy: 0.8701\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 1s 28ms/step - loss: 0.2310 - accuracy: 0.9150 - val_loss: 0.3776 - val_accuracy: 0.8192\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 0.2020 - accuracy: 0.9363 - val_loss: 0.4046 - val_accuracy: 0.8305\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 1s 28ms/step - loss: 0.1962 - accuracy: 0.9462 - val_loss: 0.5096 - val_accuracy: 0.8136\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 1s 29ms/step - loss: 0.2042 - accuracy: 0.9306 - val_loss: 0.3777 - val_accuracy: 0.8362\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 0.1944 - accuracy: 0.9419 - val_loss: 0.5694 - val_accuracy: 0.7458\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 1s 28ms/step - loss: 0.1955 - accuracy: 0.9391 - val_loss: 0.3084 - val_accuracy: 0.8757\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 1s 27ms/step - loss: 0.1794 - accuracy: 0.9504 - val_loss: 0.6679 - val_accuracy: 0.7740\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 0.2024 - accuracy: 0.9334 - val_loss: 0.3594 - val_accuracy: 0.8475\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 0.1610 - accuracy: 0.9618 - val_loss: 0.4774 - val_accuracy: 0.8249\n",
      "Time taken: 43.3 seconds\n"
     ]
    }
   ],
   "source": [
    "# train the neural network on training data set\n",
    "# batch_size (32) controls the size of each group of data to pass through the network. \n",
    "\n",
    "time1 = time.time()   # to measure time taken\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=EPOCHS, batch_size=32)\n",
    "print('Time taken: {:.1f} seconds'.format(time.time() - time1))   # to measure time taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0NicG85H1hl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LA1TugnpH1hl"
   },
   "source": [
    "### Step 8: Evaluating your model on your test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "207w4HO_H1hl"
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVjWkFsoH1hm",
    "outputId": "da4a65c8-9213-463a-b1ab-82e095ca7473"
   },
   "outputs": [],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "pred_prob = model.predict(testX, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDhO39EwH1hm",
    "outputId": "3f5ea01e-0751-4945-b6e4-4d87ccb10c72"
   },
   "outputs": [],
   "source": [
    "pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TjnaEkNDH1hn",
    "outputId": "81da7694-d14b-4e2b-edf4-8ac5db2be41c"
   },
   "outputs": [],
   "source": [
    "9.25910056e-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gfARUd9H1hn",
    "outputId": "0c1313d2-4073-4e47-ee0d-5b1992c3f2ad"
   },
   "outputs": [],
   "source": [
    "9.6146774e-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqeLZUuHH1hn"
   },
   "outputs": [],
   "source": [
    "# testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-gWgwFmH1ho"
   },
   "source": [
    "### Convert testY and y_pred into 1's and 0 for classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7DmFQsjH1ho"
   },
   "outputs": [],
   "source": [
    "# Note: buildings -> 1 and forest -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DBLMzCzH1ho"
   },
   "outputs": [],
   "source": [
    "test_y = [ np.argmax(i)  for i in testY]\n",
    "pred_y = [ np.argmax(i)  for i in pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRIqTRg0H1ho"
   },
   "outputs": [],
   "source": [
    "# test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tct3USbgH1hp",
    "outputId": "8c7e6162-445b-4a70-a3ae-86f86245949b"
   },
   "outputs": [],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2Az8TLkH1hp"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_metrix(y_true, y_pred,classes,\n",
    "                         normalize=False,\n",
    "                         title='Confusion Matrix',\n",
    "                         cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Objective\n",
    "    ----------\n",
    "    plot confussion matrix, classification report and accuracy score\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "\n",
    "    y_pred : array-like of shape (n_samples,)\n",
    "        Estimated targets as returned by a classifier.\n",
    "    \n",
    "    classes : list\n",
    "        List of labels to index the matrix\n",
    "        \n",
    "    title : title for matrix\n",
    "    cmap : colormap for matrix \n",
    "    \n",
    "    returns \n",
    "    ----------\n",
    "   all accruacy matrix \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    \n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized Confusion Matrix\")\n",
    "    else:\n",
    "        print(\"Confusion Matrix, Without Normalisation\")\n",
    "\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest',cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks,classes,rotation=35)\n",
    "    plt.yticks(tick_marks,classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() /2.\n",
    "    \n",
    "    for i , j in itertools.product(range(cm.shape[0]), range(cm.shape[0])):\n",
    "        plt.text(j, i, format(cm[i,j], fmt),\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    # plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print('Classification report')\n",
    "    print(classification_report(y_true,y_pred))\n",
    "    \n",
    "    print(\"-----------------------------------------------------\")\n",
    "    acc= accuracy_score(y_true,y_pred)\n",
    "    print(\"Accuracy of the model: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyVfoLGNH1hp",
    "outputId": "26aa7162-975a-4897-acf6-977612a1d5c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_metrix(test_y, pred_y,classes=[\"Forest: 0\",\"Buildings: 1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJqB_fgnH1hq"
   },
   "source": [
    "### Step 9: Generate Plots for acc and val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZLMjjR9H1hq",
    "outputId": "70f181ec-23e8-4da9-bfff-0f3bce365dee"
   },
   "outputs": [],
   "source": [
    "# plot the training and validation loss\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize = [10,8])\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"ANN: Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch #\", weight=\"bold\")\n",
    "plt.ylabel(\"Loss\", weight=\"bold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5De4qVQ4H1hq",
    "outputId": "927e59f6-b4d9-40e8-d17b-b4419b816230"
   },
   "outputs": [],
   "source": [
    "# plot the training and validation accuracy\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize = [10,8])\n",
    "plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"ANN: Training and Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch #\", weight=\"bold\")\n",
    "plt.ylabel(\"Accuracy\", weight=\"bold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shtColjWH1hr"
   },
   "outputs": [],
   "source": [
    "# accuracy = 88%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdGFIrlQH1hr"
   },
   "source": [
    "### Step 10: Saving the train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywefocd8H1hs",
    "outputId": "a9dd0477-a510-4a8e-91cc-60a7a95b20d8"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBZCa0oJH1hs",
    "outputId": "01d4100c-f91b-4bb6-8b79-005fa827eb1d"
   },
   "outputs": [],
   "source": [
    "# save the model and label binarizer to disk\n",
    "print(\"[INFO] serializing network and label binarizer...\")\n",
    "model.save('model_ANN.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6qjHck8H1ht"
   },
   "source": [
    "### Step 11: Making predictions using your trained tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXLgFC1yH1ht"
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import cv2\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_twfMl7H1ht",
    "outputId": "4a4e76ef-4097-40fc-b5da-f734e71d2bdd"
   },
   "outputs": [],
   "source": [
    "# # load the model \n",
    "# print(\"[INFO] loading network and...\")\n",
    "# model2 = load_model(\"model_ANN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrCsgnI5H1hu"
   },
   "outputs": [],
   "source": [
    "def display_img(img):\n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "    plt.grid(b=None)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dt9L1h68H1hu",
    "outputId": "652972b9-c8d8-484c-9cea-5dc3add4ae91",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the input image and resize it to the target spatial dimensions\n",
    "width = 32\n",
    "height = 32\n",
    "\n",
    "# grab the image paths and randomly shuffle them\n",
    "testImagePaths = sorted(list(paths.list_images('test_examples')))   # test data folder with random images\n",
    "\n",
    "\n",
    "# progress bar \n",
    "with tqdm(total=len(testImagePaths)) as pbar:\n",
    "    \n",
    "    for imagePath in testImagePaths:\n",
    "        image = cv2.imread(imagePath)\n",
    "        output = image.copy()\n",
    "        image = cv2.resize(image, (width, height))\n",
    "\n",
    "        # scale the pixel values to [0, 1]\n",
    "        image = image.astype(\"float\") / 255.0\n",
    "\n",
    "        # for a simple fully-connected network, flatten the image\n",
    "        image = image.flatten()\n",
    "        image = image.reshape((1, image.shape[0]))\n",
    "\n",
    "\n",
    "        # make a prediction on the image\n",
    "        preds = model.predict(image)\n",
    "\n",
    "        # find the class label index with the largest corresponding probability\n",
    "        i = preds.argmax(axis=1)[0]\n",
    "        label = classes[i]\n",
    "        \n",
    "        label = \"{}: {:.2f}%\".format(label, preds[0][i] * 100)\n",
    "\n",
    "        \n",
    "        output = imutils.resize(output, width=400)\n",
    "        cv2.putText(output, label, (10, 25),  cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # convert img to rgb format and display in noteboo\n",
    "        img = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n",
    "        display_img(img)\n",
    "\n",
    "#         print(\"############################\")\n",
    "#         print(\"image: {}\".format(os.path.split(imagePath)[-1]))\n",
    "#         print(\"predicted label: {}\".format(label))\n",
    "#         print(\"Confidence: {}\".format(preds[0][i]))\n",
    "        \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAyLD_WRH1hv"
   },
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ML2hzEuwH1hw"
   },
   "outputs": [],
   "source": [
    "# !pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kArJAiWyH1hw"
   },
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdQAzpp_H1hw"
   },
   "outputs": [],
   "source": [
    "def predict_image(image):\n",
    "    \n",
    "    image = cv2.resize(image, (32, 32))\n",
    "\n",
    "    # scale the pixel values to [0, 1]\n",
    "    image = image.astype(\"float\") / 255.0\n",
    "\n",
    "    # for a simple fully-connected network, flatten the image\n",
    "    image = image.flatten()\n",
    "    image = image.reshape((1, image.shape[0]))\n",
    "\n",
    "    # make a prediction on the image\n",
    "    preds = model.predict(image).flatten()\n",
    "    result = dict()\n",
    "    result[\"Forest\"] = round(float(list(preds)[0]), 3)\n",
    "    result[\"Buildings\"] = round(float(list(preds)[1]), 3)\n",
    "    print(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "im = gr.inputs.Image(shape=(32,32))\n",
    "label = gr.outputs.Label(num_top_classes=2)\n",
    "\n",
    "gr.Interface(fn=predict_image, inputs=im, outputs=label, capture_session=True, title=\"ANN Demo\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FIWwwwkH1hx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Final_ANN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
